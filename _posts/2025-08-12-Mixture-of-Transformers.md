## Introduction  介绍
In recent years, multi-modal foundation models have become a groundbreaking advancement in artificial intelligence, enabling a single model to process and understand diverse data types like text, images, and even speech. These models hold immense potential for applications ranging from content creation and image generation to complex cross-modal translation tasks, where AI can interpret and generate information across different formats. However, as promising as these capabilities are, training large multi-modal models poses significant computational and scalability challenges. Unlike single-modality models, which are trained on a single data type, multi-modal models require massive datasets and an immense amount of computational power to handle the varying structures and features of different modalities.
近年来，多模态基础模型已成为人工智能的突破性进步，使单个模型能够处理和理解文本、图像甚至语音等多种数据类型。这些模型在从内容创建和图像生成到复杂的跨模式翻译任务等应用方面具有巨大的潜力，人工智能可以解释和生成不同格式的信息。然而，尽管这些功能很有前途，但训练大型多模态模型带来了重大的计算和可扩展性挑战。与在单一数据类型上训练的单模态模型不同，多模态模型需要海量数据集和巨大的计算能力来处理不同模态的不同结构和特征。

One of the biggest hurdles with traditional dense transformer models is the “one-size-fits-all” approach. These models apply the same parameters and training techniques across different modalities, which often leads to inefficient resource usage and conflicting training dynamics. For example, processing text, images, and speech within the same model can create bottlenecks that slow down training and increase computational costs. As the demand for multi-modal AI grows, a more efficient solution is essential to balance high performance with reasonable computational costs.
传统密集变压器模型的最大障碍之一是“一刀切”的方法。这些模型在不同的模式中应用相同的参数和训练技术，这通常会导致资源使用效率低下和训练动态冲突。例如，在同一模型中处理文本、图像和语音可能会产生瓶颈，从而减慢训练速度并增加计算成本。随着对多模态人工智能的需求不断增长，更高效的解决方案对于平衡高性能与合理的计算成本至关重要。

Enter the Mixture-of-Transformers (MoT), a new sparse architecture designed specifically to tackle these challenges. Unlike dense models, MoT introduces a unique way of processing multi-modal inputs through “modality-specific parameter decoupling,” where separate sets of parameters are used for each data type. This not only reduces computational load but also allows each modality to be optimized individually, enhancing efficiency without sacrificing quality. In this blog, we’ll explore how the Mixture-of-Transformers model redefines multi-modal foundation models, delivering high performance at a fraction of the computational cost.
变压器混合 （MoT） 是一种专为应对这些挑战而设计的新型稀疏架构。与密集模型不同，MoT 引入了一种通过“特定模态参数解耦”来处理多模态输入的独特方法，其中每种数据类型使用单独的参数集。这不仅减少了计算负载，还允许单独优化每种模式，从而在不牺牲质量的情况下提高效率。在本博客中，我们将探讨 Mix-of-Transformers 模型如何重新定义多模态基础模型，以极低的计算成本提供高性能。

The Problem with Traditional Dense Models in Multi-Modal Learning
传统密集模型在多模态学习中存在的问题
High Computational Costs: Traditional dense transformer models apply the same parameters and architectures across all data modalities (e.g., text, image, speech). This leads to inefficient usage of computational resources, as each modality has unique characteristics that require different optimizations​.
计算成本高： 传统的密集转换器模型在所有数据模态（例如文本、图像、语音）上应用相同的参数和架构。这导致计算资源的使用效率低下，因为每种模态都有独特的特征，需要不同的优化。
Conflicting Training Dynamics: When dense models process multiple modalities together, they can encounter conflicting training dynamics. This means that optimizing the model for one modality can negatively impact another, reducing the model’s overall effectiveness and requiring even more resources for fine-tuning​
冲突的训练动态 ：当密集模型一起处理多种模态时，它们可能会遇到冲突的训练动态。这意味着针对一种模式优化模型可能会对另一种模式产生负面影响，从而降低模型的整体有效性，并需要更多的资源进行微调
Resource Intensity for Scaling: To achieve high performance on multi-modal tasks, dense models require massive amounts of data and computational power. For instance, Chameleon, a dense multi-modal model, requires 9.2 trillion tokens to achieve performance comparable to single-modality models trained on far fewer tokens​
扩展的资源强度 ：为了在多模态任务上实现高性能，密集模型需要大量数据和计算能力。例如，Chameleon 是一个密集的多模态模型，需要 9.2 万亿个代币才能实现与在更少的代币上训练的单模态模型相当的性能
Uniform Processing Across Modalities: Dense models process all inputs uniformly, treating text, images, and speech as similar types of data despite their structural differences. This uniform approach can lead to inefficiencies and limits the model’s ability to excel in individual modalities.
跨模态的统一处理 ：密集模型统一处理所有输入，将文本、图像和语音视为相似类型的数据，尽管它们存在结构差异。这种统一的方法可能会导致效率低下，并限制模型在各个模式中脱颖而出的能力。
Press enter or click to view image in full size

Liang, Weixin, et al. “Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models.” arXiv preprint arXiv:2411.04996 (2024).
Liang， Weixin， et al. “Mix-of-Transformers： A Sparse and Scalable Architecture for Multi-Modal Foundation Models.”arXiv 预印本 arXiv：2411.04996 （2024）。
As shown in the figure above the model processes a mixed sequence of text and image tokens (like words and image segments) as a single stream. It uses an autoregressive approach, predicting the next token in the sequence, whether it’s text or part of an image. Images are broken down into discrete tokens (like a list of image pieces) using a pre-trained model, so the model can handle images similarly to text.
如上图所示，该模型将文本和图像标记的混合序列（如单词和图像段）作为单个流进行处理。它使用自回归方法 ，预测序列中的下一个标记，无论是文本还是图像的一部分。使用预训练模型将图像分解为离散标记（如图像片段列表），因此模型可以像处理文本一样处理图像。

As the model processes data across layers, it naturally starts to group different types of tokens (text and image) into separate clusters in its feature space, especially in the deeper layers. This shows the model’s growing ability to distinguish between data types.Even though this model treats all data uniformly, it still learns to differentiate text from images internally, suggesting that adding modality-specific components (like in Mixture-of-Transformers) could further enhance this separation and improve efficiency.
当模型跨层处理数据时，它自然会开始将不同类型的标记（文本和图像）分组到其特征空间中的单独集群中，尤其是在更深的层中。这表明模型区分数据类型的能力不断增强。尽管该模型统一处理所有数据，但它仍然学会在内部区分文本和图像，这表明添加特定于模态的组件（如在 Mix-of-Transformers 中）可以进一步增强这种分离并提高效率。

Growing Demand for a Sparse Architecture: Sparse architectures, like the Mixture-of-Experts (MoE) model, aim to address these issues by activating only a subset of model parameters per modality. However, MoE models face challenges like imbalanced expert utilization, requiring complex routing algorithms that add further computational load​.
对稀疏架构的需求不断增长 ：稀疏架构（如专家混合 （MoE） 模型）旨在通过仅激活每个模态的模型参数子集来解决这些问题。然而，MoE 模型面临着专家利用率不平衡等挑战，需要复杂的路由算法，从而进一步增加计算负载。
In response to these challenges, the Mixture-of-Transformers (MoT) architecture was developed as a sparse, multi-modal model. By decoupling parameters based on modality, MoT reduces computational requirements and mitigates the training conflicts seen in dense architectures. The following sections will explore MoT’s architecture and its impressive efficiency in handling text, image, and speech tasks without compromising performance​(mixture of transformers).
为了应对这些挑战，Mix-of-Transformers （MoT） 架构被开发为稀疏的多模态模型。通过基于模态解耦参数，MoT 降低了计算需求并减轻了密集架构中出现的训练冲突。以下部分将探讨 MoT 的架构及其在处理文本、图像和语音任务而不影响性能（转换器混合）方面的令人印象深刻的效率。